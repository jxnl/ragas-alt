{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"RAG Evals: Retrieval Augmented Generation","text":"<p>Provides simple, modular primitives for evaluating RAG systems using LLMs.</p>"},{"location":"#philosophy","title":"Philosophy","text":"<p>From the creator of Instructor: RAG Evals is built on the principle that RAG evaluation doesn't need to be complex. While many RAG evaluation frameworks require extensive infrastructure and setup, this project aims to provide:</p> <ul> <li>Simple Primitives: Just the essential building blocks you need</li> <li>Flexible Implementation: Easily customizable prompts and evaluation logic</li> <li>No Infrastructure Lock-in: Run evaluations in your existing environment</li> <li>LLM Flexibility: Use any LLM that works with Instructor</li> </ul> <p>Similar to Instructor, RAG Evals focuses on providing the fundamental tools that companies need without imposing unnecessary complexity or infrastructure requirements.</p>"},{"location":"#overview","title":"Overview","text":"<p>RAG Evals helps you evaluate the quality of your RAG (Retrieval Augmented Generation) systems across several crucial dimensions:</p> <ul> <li>How truthful are your answers? (Faithfulness)</li> <li>Are your retrieved chunks relevant to the question? (Context Precision)</li> </ul>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>LLM-Powered Evaluation: Uses language models to perform nuanced assessments</li> <li>Structured Output: Leverages Instructor and Pydantic for type-safe results</li> <li>Modifiable Prompts: All evaluation logic is in prompts you can easily customize</li> <li>Parallel Execution: Run evaluations concurrently for better performance</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>import instructor\nfrom rag_evals.score_faithfulness import Faithfulness\nfrom rag_evals.score_precision import ChunkPrecision\n\n# Initialize with your preferred LLM\nclient = instructor.from_provider(\"openai/gpt-4o-mini\")\n\n# Evaluate Faithfulness\nquestion = \"What are the benefits of exercise?\"\nanswer = \"Regular exercise improves cardiovascular health and increases strength.\"\ncontext = [\n    \"Regular physical activity improves heart health and circulation.\",\n    \"Weight training builds muscle strength and increases bone density.\",\n    \"The earliest Olympic games were held in Ancient Greece.\"\n]\n\n# Run evaluations\nfaithfulness_result = Faithfulness.grade(\n    question=question,\n    answer=answer,\n    context=context,\n    client=client\n)\n\nprint(f\"Overall Faithfulness Score: {faithfulness_result.overall_faithfulness_score}\")\nfor statement in faithfulness_result.statements:\n    print(f\"- {statement.statement}: {'Supported' if statement.is_supported else 'Unsupported'}\")\n    if statement.is_supported:\n        print(f\"  Supported by chunks: {statement.supporting_chunk_ids}\")\n\n# Evaluate Context Precision\nprecision_result = ChunkPrecision.grade(\n    question=question,\n    answer=answer,\n    context=context,\n    client=client\n)\n\nprint(f\"Overall Precision Score: {precision_result.avg_score}\")\nfor chunk in precision_result.graded_chunks:\n    print(f\"- Chunk {chunk.id_chunk}: {'Relevant' if chunk.score else 'Not Relevant'}\")\n</code></pre> <p>For more information, check out the metrics documentation and the usage guide.</p>"},{"location":"api/","title":"API Reference","text":"<p>This section provides detailed API documentation for the RAG Evals library.</p>"},{"location":"api/#core-classes","title":"Core Classes","text":""},{"location":"api/#contextevaluation","title":"ContextEvaluation","text":"<p><code>ContextEvaluation</code> is the base class for all context-based evaluations in RAG Evals.</p> <pre><code>class ContextEvaluation(BaseModel):\n    \"\"\"Base class for context-based evaluations that handles common patterns\n    including grading question, and optional answers against a context that is enumerated   \n    with an id.\n\n    This class is designed to be used as a base class for specific evaluation classes.\n    It provides a common interface for evaluating questions and answers against a context.\n    \"\"\"\n\n    prompt: str\n    examples: list[Any] | None = None\n    response_model: type[BaseModel]\n    chunk_template: str = dedent(\"\"\"\n        &lt;evaluation&gt;\n            {% if examples is not none %}\n            &lt;examples&gt;\n                {% for example in examples %}\n                &lt;example&gt;\n                    {{ example }}\n                &lt;/example&gt;\n                {% endfor %}\n            {% endif %}\n            &lt;question&gt;{{ question }}&lt;/question&gt;\n            {% if answer is not none %}\n            &lt;answer&gt;{{ answer }}&lt;/answer&gt;\n            {% endif %}\n            &lt;context&gt;\n                {% for chunk in context %}\n                &lt;chunk id=\"{{ chunk.id }}\"&gt;\n                    {{ chunk.chunk }}\n                &lt;/chunk&gt;\n                {% endfor %}\n            &lt;/context&gt;\n        &lt;/evaluation&gt;\n    \"\"\")\n</code></pre>"},{"location":"api/#methods","title":"Methods","text":""},{"location":"api/#grade","title":"grade","text":"<pre><code>def grade(\n    self,\n    question: str,\n    answer: str | None,\n    context: list[Any],\n    client: Instructor,\n) -&gt; BaseModel:\n    \"\"\"Run an evaluation of a question and optional answer against provided context chunks.\n\n    Args:\n        question (str): The question being evaluated.\n        answer (Optional[str]): The answer to evaluate, if available. Can be None.\n        context (List[Any]): List of context chunks to evaluate against.\n        client (Instructor): An initialized Instructor client instance.\n\n    Returns:\n        BaseModel: An instance of the response_model containing the structured evaluation results.\n    \"\"\"\n</code></pre>"},{"location":"api/#agrade","title":"agrade","text":"<pre><code>async def agrade(\n    self,\n    question: str,\n    answer: str | None,\n    context: list[Any],\n    client: AsyncInstructor,\n) -&gt; BaseModel:\n    \"\"\"Run an evaluation of a question and optional answer against provided context chunks asynchronously.\n\n    Args:   \n        question (str): The question being evaluated.   \n        answer (Optional[str]): The answer to evaluate, if available. Can be None.\n        context (List[Any]): List of context chunks to evaluate against.\n        client (AsyncInstructor): An initialized AsyncInstructor client instance.\n\n    Returns:\n        BaseModel: An instance of the response_model containing the structured evaluation results.\n    \"\"\"\n</code></pre>"},{"location":"api/#contextvalidationmixin","title":"ContextValidationMixin","text":"<p>A mixin class that ensures the integrity of chunk references in RAG evaluations by validating that all chunk IDs correspond to actual context chunks.</p> <pre><code>class ContextValidationMixin:\n    \"\"\"Mixin class that ensures the integrity of chunk references in RAG evaluations\n    by validating that all chunk IDs correspond to actual context chunks.\"\"\"\n\n    @field_validator('graded_chunks')\n    @classmethod\n    def validate_chunks_against_context(cls, chunks: list[Any], info: ValidationInfo) -&gt; list[Any]:\n        \"\"\"Validate and process chunk IDs against context chunks.\"\"\"\n</code></pre>"},{"location":"api/#chunkscore","title":"ChunkScore","text":"<p>Represents a score for a single context chunk.</p> <pre><code>class ChunkScore(BaseModel):\n    id_chunk: int\n    score: float = Field(ge=0.0, le=1.0, description=\"Score from 0-1 indicating the precision of the chunk, lower is worse\")\n</code></pre>"},{"location":"api/#chunkbinaryscore","title":"ChunkBinaryScore","text":"<p>Represents a binary (pass/fail) score for a single context chunk.</p> <pre><code>class ChunkBinaryScore(BaseModel):\n    id_chunk: int\n    score: bool = Field(description=\"Whether the chunk is passed or failed\")\n</code></pre>"},{"location":"api/#chunkgraded","title":"ChunkGraded","text":"<p>Container for a list of graded chunks with a continuous score.</p> <pre><code>class ChunkGraded(BaseModel, ContextValidationMixin):\n    graded_chunks: list[ChunkScore]\n\n    @property \n    def avg_score(self) -&gt; float:\n        return sum(chunk.score for chunk in self.graded_chunks) / len(self.graded_chunks)\n</code></pre>"},{"location":"api/#chunkgradedbinary","title":"ChunkGradedBinary","text":"<p>Container for a list of graded chunks with a binary score.</p> <pre><code>class ChunkGradedBinary(BaseModel, ContextValidationMixin):\n    graded_chunks: list[ChunkBinaryScore]\n\n    @property \n    def avg_score(self) -&gt; float:\n        return sum(chunk.score for chunk in self.graded_chunks) / len(self.graded_chunks)\n</code></pre>"},{"location":"api/#faithfulness-module","title":"Faithfulness Module","text":""},{"location":"api/#faithfulnessresult","title":"FaithfulnessResult","text":"<p>The result of a faithfulness evaluation.</p> <pre><code>class FaithfulnessResult(BaseModel):\n    statements: list[StatementEvaluation] = Field(description=\"A list of all statements extracted from the answer and their evaluation.\")\n\n    @property\n    def overall_faithfulness_score(self) -&gt; float:\n        if not self.statements:\n            return 0.0\n        supported_statements = sum(s.is_supported for s in self.statements)\n        return supported_statements / len(self.statements)\n</code></pre>"},{"location":"api/#statementevaluation","title":"StatementEvaluation","text":"<p>Represents the evaluation of a single statement extracted from an answer.</p> <pre><code>class StatementEvaluation(BaseModel):\n    statement: str = Field(description=\"An individual claim extracted from the generated answer.\")\n    is_supported: bool = Field(description=\"Is this statement supported by the provided context chunks?\")\n    supporting_chunk_ids: Optional[list[int]] = Field(\n        default=None, \n        description=\"A list of chunk IDs (0-indexed integers) from the provided context that support this statement.\"\n    )\n</code></pre>"},{"location":"api/#faithfulness","title":"Faithfulness","text":"<p>The main faithfulness evaluator.</p> <pre><code>Faithfulness = base.ContextEvaluation(\n    prompt=\"\"\"\n    You are an expert evaluator tasked with assessing the factual faithfulness of a generated answer to its provided context...\n    \"\"\",\n    response_model=FaithfulnessResult\n)\n</code></pre>"},{"location":"api/#precision-module","title":"Precision Module","text":""},{"location":"api/#chunkprecision","title":"ChunkPrecision","text":"<p>The main context precision evaluator.</p> <pre><code>ChunkPrecision = base.ContextEvaluation(\n    prompt = \"\"\"\n    You are an expert evaluator assessing if a specific retrieved context chunk was utilized in generating a given answer...\n    \"\"\", \n    response_model = base.ChunkGradedBinary\n)\n</code></pre>"},{"location":"api/#usage-examples","title":"Usage Examples","text":"<p>For complete usage examples, see:</p> <ul> <li>Basic Usage</li> <li>Examples</li> <li>Customization</li> </ul>"},{"location":"metrics/","title":"Understanding RAG Evaluation Metrics","text":"<p>When building Retrieval Augmented Generation (RAG) systems, it's crucial to measure performance across different dimensions. This documentation explains the core metrics implemented in RAG Evals.</p>"},{"location":"metrics/#implemented-metrics","title":"Implemented Metrics","text":"<p>RAG Evals currently implements these key evaluation metrics:</p>"},{"location":"metrics/#faithfulness-answer-grounding","title":"Faithfulness (Answer Grounding)","text":"<p>Measures whether the generated answer contains only factual claims supported by the retrieved context. It helps detect hallucinations (made-up information).</p> <ul> <li>What it measures: Is the system's generated answer factually supported by the retrieved context? It checks if the answer invents information (hallucinates) or contradicts the information found in the retrieved context.</li> <li>Focus: The relationship is (Generated Answer \u2192 Retrieved Context).</li> <li>Input: Question, Answer, Retrieved Context</li> <li>Output: Statement-level breakdown of which claims are supported, with specific attribution to context chunks</li> </ul> <p>Learn more about Faithfulness</p>"},{"location":"metrics/#context-precision-chunk-relevancy","title":"Context Precision (Chunk Relevancy)","text":"<p>Evaluates whether each individual retrieved context chunk is relevant to the original question.</p> <ul> <li>What it measures: For each individual retrieved chunk of context, how relevant is its content to the user's original question, regardless of whether that chunk was actually used in the final generated answer?</li> <li>Focus: The relationship is (Individual Retrieved Chunk \u2192 User's Question).</li> <li>Input: Question, Answer, Retrieved Context</li> <li>Output: Binary score for each chunk indicating whether it is relevant to the question</li> </ul> <p>Learn more about Context Precision</p>"},{"location":"metrics/#answer-relevance","title":"Answer Relevance","text":"<p>Evaluates how well the generated answer addresses the original question.</p> <ul> <li>What it measures: How well does the generated answer address the user's original question, regardless of factual accuracy or context support?</li> <li>Focus: The relationship is (Generated Answer \u2192 User's Question).</li> <li>Input: Question, Answer, Context (though context isn't used in the evaluation)</li> <li>Output: Multi-dimensional score assessing topical match, completeness, and conciseness</li> </ul> <p>Learn more about Answer Relevance</p>"},{"location":"metrics/#understanding-the-difference-between-metrics","title":"Understanding the Difference Between Metrics","text":"<p>It's important to understand how these metrics differ from each other:</p>"},{"location":"metrics/#faithfulness-vs-context-precision","title":"Faithfulness vs. Context Precision","text":"<ul> <li>Faithfulness starts with the answer and verifies if each statement in it is supported by the context</li> <li>Context Precision starts with each context chunk and determines if it is relevant to the original question</li> </ul>"},{"location":"metrics/#faithfulness-vs-answer-relevance","title":"Faithfulness vs. Answer Relevance","text":"<ul> <li>Faithfulness measures how well the answer is grounded in the context (factual correctness)</li> <li>Answer Relevance measures how well the answer addresses the question (response appropriateness)</li> </ul>"},{"location":"metrics/#context-precision-vs-answer-relevance","title":"Context Precision vs. Answer Relevance","text":"<ul> <li>Context Precision evaluates the retrieval component by checking chunk relevance to the question</li> <li>Answer Relevance evaluates the generation component by checking answer responsiveness to the question</li> </ul>"},{"location":"metrics/#systematic-decomposition-of-rag-evaluations","title":"Systematic Decomposition of RAG Evaluations","text":"<p>For a comprehensive understanding of how different evaluation metrics relate to each other and form a complete evaluation framework, see our Systematic Decomposition of RAG Evaluations guide.</p> <p>This framework helps you understand the relationships between questions, context chunks, and answers, and how different evaluation metrics assess these relationships.</p>"},{"location":"metrics/#future-metrics","title":"Future Metrics","text":"<p>Future releases of RAG Evals plan to include:</p> <ul> <li>Context Recall: Evaluates if the retrieved context contains all information needed for an ideal answer</li> <li>Chunk Utility: Assesses whether each retrieved chunk was actually used in generating the answer</li> <li>Citation Quality: Measures how well the system attributes information to sources</li> <li>Response Coherence: Evaluates the overall structure and flow of generated answers</li> </ul> <p>For more details on best practices when using these metrics, see our Evaluation Best Practices guide.</p>"},{"location":"metrics/faithfulness/","title":"Faithfulness Evaluation","text":"<p>Faithfulness is one of the most critical metrics for RAG evaluation. It measures whether the generated answer contains only factual claims that are supported by the retrieved context.</p>"},{"location":"metrics/faithfulness/#what-faithfulness-measures","title":"What Faithfulness Measures","text":"<ul> <li>Definition: Faithfulness measures if the system's generated answer is factually supported by the retrieved context. </li> <li>Focus: The relationship is (Generated Answer \u2192 Retrieved Context).</li> <li>Purpose: To detect hallucinations (made-up information) or contradictions between the answer and the context.</li> </ul>"},{"location":"metrics/faithfulness/#how-it-works","title":"How It Works","text":"<p>The Faithfulness evaluator:</p> <ol> <li>Breaks down the answer into individual, verifiable statements</li> <li>For each statement, determines if it is supported by the provided context</li> <li>Identifies which specific context chunks support each statement</li> <li>Calculates an overall faithfulness score based on the proportion of supported statements</li> </ol>"},{"location":"metrics/faithfulness/#implementation-details","title":"Implementation Details","text":"<p>The faithfulness implementation uses a structured approach:</p> <pre><code>from rag_evals.score_faithfulness import Faithfulness\n\nfaithfulness_result = Faithfulness.grade(\n    question=question,\n    answer=answer,\n    context=context,\n    client=client\n)\n</code></pre>"},{"location":"metrics/faithfulness/#response-model","title":"Response Model","text":"<p>The <code>FaithfulnessResult</code> class contains:</p> <pre><code>class StatementEvaluation(BaseModel):\n    statement: str  # An individual claim extracted from the answer\n    is_supported: bool  # Whether the statement is supported by context\n    supporting_chunk_ids: Optional[list[int]]  # IDs of supporting chunks\n\nclass FaithfulnessResult(BaseModel):\n    statements: list[StatementEvaluation]  # All evaluated statements\n\n    @property\n    def overall_faithfulness_score(self) -&gt; float:\n        # Calculates the proportion of supported statements\n</code></pre>"},{"location":"metrics/faithfulness/#example-output","title":"Example Output","text":"<pre><code># Result example\nfaithfulness_result = FaithfulnessResult(\n    statements=[\n        StatementEvaluation(\n            statement=\"Regular exercise improves cardiovascular health\", \n            is_supported=True, \n            supporting_chunk_ids=[0]\n        ),\n        StatementEvaluation(\n            statement=\"Exercise increases strength\", \n            is_supported=True, \n            supporting_chunk_ids=[1]\n        )\n    ]\n)\n\n# Overall score: 1.0 (both statements are supported)\n</code></pre>"},{"location":"metrics/faithfulness/#customizing-the-evaluation","title":"Customizing the Evaluation","text":"<p>You can customize the faithfulness prompt to adjust the evaluation criteria:</p> <pre><code>from rag_evals.score_faithfulness import Faithfulness\nfrom rag_evals import base\n\n# Access the original prompt\noriginal_prompt = Faithfulness.prompt\n\n# Create a customized evaluator with a modified prompt\nfrom rag_evals.score_faithfulness import FaithfulnessResult\n\nCustomFaithfulness = base.ContextEvaluation(\n    prompt=\"Your custom prompt here...\",\n    response_model=FaithfulnessResult\n)\n</code></pre>"},{"location":"metrics/faithfulness/#considerations-when-using-faithfulness","title":"Considerations When Using Faithfulness","text":"<ul> <li>Strict vs. Lenient Evaluation: Consider how strict you want the evaluation to be. Should inferences and logical conclusions be considered faithful if they're not explicitly stated in the context?</li> <li>Granularity of Statements: The way statements are broken down affects the score. More granular statements provide a more detailed evaluation.</li> <li>Context Relevance: Faithfulness only measures if the answer is supported by the provided context, not whether the context is relevant to the question.</li> </ul>"},{"location":"metrics/faithfulness/#best-practices","title":"Best Practices","text":"<ul> <li>Use faithfulness alongside other metrics like context precision for a complete evaluation</li> <li>Ensure your LLM for evaluation is capable of detailed statement-by-statement analysis</li> <li>Review statement breakdowns to understand where hallucinations occur</li> <li>Consider the confidence level of each statement's support</li> </ul> <p>For implementation examples, see the usage examples.</p>"},{"location":"metrics/precision/","title":"Context Precision Evaluation","text":"<p>Context Precision (also known as Chunk Relevancy) measures whether each retrieved context chunk is relevant to the original question. This metric helps assess the efficiency of your retrieval system.</p>"},{"location":"metrics/precision/#what-context-precision-measures","title":"What Context Precision Measures","text":"<ul> <li>Definition: Context Precision evaluates for each individual retrieved chunk of context, how relevant its content is to the user's original question, regardless of whether that chunk was actually used in the final generated answer.</li> <li>Focus: The relationship is (Individual Retrieved Chunk \u2192 User's Question).</li> <li>Purpose: To determine if the retriever is fetching chunks that are relevant to the user's query. If many chunks are retrieved but aren't relevant to the question, the retrieval might be inefficient.</li> </ul>"},{"location":"metrics/precision/#how-it-works","title":"How It Works","text":"<p>The Context Precision evaluator:</p> <ol> <li>Examines each context chunk independently</li> <li>Determines if the information in the chunk is relevant to answering the original question</li> <li>Assigns a binary score (relevant/not relevant) to each chunk</li> <li>Calculates an overall precision score based on the proportion of relevant chunks</li> </ol>"},{"location":"metrics/precision/#implementation-details","title":"Implementation Details","text":"<p>The context precision implementation uses a straightforward approach:</p> <pre><code>from rag_evals.score_precision import ChunkPrecision\n\nprecision_result = ChunkPrecision.grade(\n    question=question,\n    answer=answer,\n    context=context,\n    client=client\n)\n</code></pre>"},{"location":"metrics/precision/#response-model","title":"Response Model","text":"<p>The ChunkPrecision evaluator uses the base <code>ChunkGradedBinary</code> class:</p> <pre><code>class ChunkBinaryScore(BaseModel):\n    id_chunk: int  # ID of the chunk being evaluated\n    score: bool  # Whether the chunk is relevant (True) or not (False)\n\nclass ChunkGradedBinary(BaseModel, ContextValidationMixin):\n    graded_chunks: list[ChunkBinaryScore]  # All evaluated chunks\n\n    @property \n    def avg_score(self) -&gt; float:\n        # Calculates the proportion of relevant chunks\n</code></pre>"},{"location":"metrics/precision/#example-output","title":"Example Output","text":"<pre><code># Result example\nprecision_result = ChunkGradedBinary(\n    graded_chunks=[\n        ChunkBinaryScore(id_chunk=0, score=True),   # Chunk is relevant to the question\n        ChunkBinaryScore(id_chunk=1, score=True),   # Chunk is relevant to the question\n        ChunkBinaryScore(id_chunk=2, score=False),  # Chunk is not relevant to the question\n    ]\n)\n\n# Overall score: 0.6667 (2/3 chunks were relevant)\n</code></pre>"},{"location":"metrics/precision/#customizing-the-evaluation","title":"Customizing the Evaluation","text":"<p>You can customize the context precision prompt to adjust the criteria for what makes a chunk \"relevant\":</p> <pre><code>from rag_evals.score_precision import ChunkPrecision\nfrom rag_evals import base\n\n# Access the original prompt\noriginal_prompt = ChunkPrecision.prompt\n\n# Create a customized evaluator with a modified prompt\nCustomPrecision = base.ContextEvaluation(\n    prompt=\"Your custom prompt here...\",\n    response_model=base.ChunkGradedBinary\n)\n</code></pre>"},{"location":"metrics/precision/#context-precision-vs-chunk-utility","title":"Context Precision vs. Chunk Utility","text":"<p>It's important to understand the difference between:</p> <ul> <li>Context Precision: Measures if a chunk is relevant to the question (regardless of whether it was used in the answer)</li> <li>Chunk Utility: Measures if a chunk was actually used in generating the answer</li> </ul> <p>A chunk might be highly relevant to the question but not used in the answer, or it might be used in the answer despite having low relevance to the question.</p>"},{"location":"metrics/precision/#considerations-when-using-context-precision","title":"Considerations When Using Context Precision","text":"<ul> <li>Partial Relevance: Consider how to score chunks that are only partially relevant to the question</li> <li>Topical vs. Factual Relevance: A chunk might be topically relevant but not contain the specific facts needed</li> <li>Question Decomposition: For complex questions with multiple parts, chunks may be relevant to only some parts</li> </ul>"},{"location":"metrics/precision/#best-practices","title":"Best Practices","text":"<ul> <li>Use context precision alongside other metrics like faithfulness for a complete evaluation</li> <li>Analyze chunks marked as \"not relevant\" to improve your retrieval system</li> <li>Consider both precision and recall metrics for a comprehensive view of retrieval performance</li> <li>Try different chunk sizes to find the optimal granularity for your use case</li> </ul> <p>For implementation examples, see the usage examples.</p>"},{"location":"metrics/relevance/","title":"Answer Relevance Evaluation","text":"<p>Answer Relevance evaluates how well the generated answer addresses the original question. This metric helps assess the quality of the answer from the user's perspective, focusing on how useful and on-topic the response is.</p>"},{"location":"metrics/relevance/#what-answer-relevance-measures","title":"What Answer Relevance Measures","text":"<ul> <li>Definition: Answer Relevance assesses how well the generated answer addresses the original question, regardless of whether the information in the answer is factually accurate or supported by the context.</li> <li>Focus: The relationship is (Answer \u2192 Question).</li> <li>Purpose: To determine if the answer is responsive to the user's information need, even if the retrieval or generation components have issues.</li> </ul>"},{"location":"metrics/relevance/#how-it-works","title":"How It Works","text":"<p>The Answer Relevance evaluator:</p> <ol> <li>Examines the question and answer independently of the context</li> <li>Evaluates how well the answer addresses the question along multiple dimensions</li> <li>Provides a holistic score indicating the relevance of the answer to the question</li> </ol>"},{"location":"metrics/relevance/#implementation-details","title":"Implementation Details","text":"<p>The answer relevance implementation evaluates three key dimensions:</p> <pre><code>from rag_evals.score_relevance import AnswerRelevance\n\nrelevance_result = AnswerRelevance.grade(\n    question=question,\n    answer=answer,\n    context=context,  # Note: context is not used in this evaluation\n    client=client\n)\n</code></pre>"},{"location":"metrics/relevance/#response-model","title":"Response Model","text":"<p>The <code>RelevanceScore</code> class contains:</p> <pre><code>class RelevanceScore(BaseModel):\n    \"\"\"Represents the evaluation of answer relevance to the question.\"\"\"\n    overall_score: float  # Overall relevance score from 0-1\n    topical_match: float  # How well the answer's topic matches the question\n    completeness: float  # How completely the answer addresses all aspects\n    conciseness: float  # How concise and focused the answer is\n    reasoning: str  # Explanation of the reasoning behind the scores\n</code></pre>"},{"location":"metrics/relevance/#evaluation-dimensions","title":"Evaluation Dimensions","text":"<ol> <li>Topical Match: How well does the answer's subject matter align with what was asked?</li> <li>Completeness: How thoroughly does the answer address all aspects of the question?</li> <li>Conciseness: How focused is the answer without irrelevant information?</li> </ol> <p>The overall score is calculated as the average of these three dimensions.</p>"},{"location":"metrics/relevance/#example-output","title":"Example Output","text":"<pre><code># Result example\nrelevance_result = RelevanceScore(\n    overall_score=0.8,\n    topical_match=0.9,\n    completeness=0.7,\n    conciseness=0.8,\n    reasoning=\"The answer directly addresses the question about meditation benefits, covering most key aspects like stress reduction and improved focus. It's concise and stays on topic with minimal tangents.\"\n)\n</code></pre>"},{"location":"metrics/relevance/#customizing-the-evaluation","title":"Customizing the Evaluation","text":"<p>You can customize the answer relevance prompt to adjust the evaluation criteria:</p> <pre><code>from rag_evals import base\nfrom rag_evals.score_relevance import RelevanceScore\n\n# Create a customized evaluator with a modified prompt\nCustomRelevance = base.ContextEvaluation(\n    prompt=\"Your custom prompt here with specific evaluation criteria...\",\n    response_model=RelevanceScore\n)\n</code></pre>"},{"location":"metrics/relevance/#answer-relevance-vs-faithfulness","title":"Answer Relevance vs. Faithfulness","text":"<p>It's important to understand the difference between:</p> <ul> <li>Answer Relevance: Measures how well the answer addresses the question, regardless of factual accuracy</li> <li>Faithfulness: Measures how well the answer is supported by the context</li> </ul> <p>For example, an answer can be highly relevant to the question but unfaithful to the context, or vice versa.</p>"},{"location":"metrics/relevance/#considerations-when-using-answer-relevance","title":"Considerations When Using Answer Relevance","text":"<ul> <li>Multi-part Questions: Consider how well the answer addresses all aspects of complex questions</li> <li>Implicit Questions: Evaluate how well the answer addresses the intent behind the question</li> <li>Overly Verbose Answers: Consider penalizing answers that include significant irrelevant information</li> <li>Different Answer Styles: Some questions may be appropriately answered with different levels of detail</li> </ul>"},{"location":"metrics/relevance/#best-practices","title":"Best Practices","text":"<ul> <li>Use answer relevance alongside other metrics like faithfulness and context precision</li> <li>Analyze low-scoring answers to understand where the response generation needs improvement</li> <li>Consider different weightings of topical match, completeness, and conciseness based on your use case</li> <li>Keep in mind that context-less answer relevance doesn't capture factual accuracy</li> </ul> <p>For implementation examples, see the usage examples.</p>"},{"location":"metrics/systematic-decomposition/","title":"Systematic Decomposition of RAG Evaluations","text":"<p>RAG systems involve three core components: the question, the retrieved context chunks, and the generated answer. By examining the relationships between these components, we can systematically decompose RAG evaluations into distinct metrics that assess different aspects of the system's performance.</p>"},{"location":"metrics/systematic-decomposition/#core-components","title":"Core Components","text":"<ul> <li>Question (Q): The user's original query</li> <li>Context Chunks (C): The pieces of information retrieved from a knowledge base</li> <li>Answer (A): The response generated based on the question and context</li> </ul>"},{"location":"metrics/systematic-decomposition/#evaluation-relationships","title":"Evaluation Relationships","text":"<p>We can express RAG evaluations in terms of relationships between these components. For each pair, we evaluate how well one component relates to another:</p> <pre><code>graph LR\n    Q[Question] --&gt; |Relevance| C[Context Chunks]\n    C --&gt; |Faithfulness| A[Answer]\n    Q --&gt; |Answer Relevance| A\n    C &lt;--&gt; |Context Utilization| A\n    Q &lt;--&gt; |Question Answering| A</code></pre> <p>Let's examine each of these relationships:</p>"},{"location":"metrics/systematic-decomposition/#1-question-context-chunks-context-relevance-cq","title":"1. Question \u2192 Context Chunks: Context Relevance (C|Q)","text":"<p>This evaluates how well the retrieved chunks relate to the question.</p> <p>Also known as:  - Context Precision - Retrieval Relevance  - Contextual Relevancy</p> <p>Metrics in this dimension:</p> <ul> <li>Context Precision: What proportion of retrieved chunks are relevant to the question?</li> <li>Context Recall: Does the retrieved context contain all the information needed to provide a complete answer?</li> <li>Context Relevance: How relevant is each specific chunk to the question?</li> </ul> <p>Example metric:  - Percentage of retrieved documents relevant to the query (Myscale +4) - In RAG Evals: <code>ChunkPrecision</code> class in <code>metrics/precision.py</code></p> <p>Example: <pre><code>Question: \"What are the health benefits of meditation?\"\nContext Chunks:\n[0] \"Regular meditation reduces stress and anxiety.\"\n[1] \"Meditation can improve focus and attention span.\"\n[2] \"The history of meditation dates back to ancient civilizations.\"\n\nEvaluation:\n- Chunks 0 and 1: Highly relevant (directly address health benefits)\n- Chunk 2: Not relevant (discusses history, not health benefits)\nContext Precision Score: 2/3 = 0.67\n</code></pre></p>"},{"location":"metrics/systematic-decomposition/#2-context-chunks-answer-faithfulnessgroundedness-ac","title":"2. Context Chunks \u2192 Answer: Faithfulness/Groundedness (A|C)","text":"<p>This evaluates how well the answer uses the provided context.</p> <p>Also known as: - Factuality - Correctness - Answer Grounding</p> <p>Metrics in this dimension:</p> <ul> <li>Faithfulness/Groundedness: Does the answer only contain information supported by the context?</li> <li>Citation Accuracy: Does the answer correctly attribute information to sources?</li> <li>Hallucination Detection: Does the answer invent information not found in the context?</li> </ul> <p>Example metric: - Ratio of statements in the answer supported by context (Deepeval +5) - In RAG Evals: <code>Faithfulness</code> class in <code>metrics/faithfulness.py</code></p> <p>Example: <pre><code>Context Chunks:\n[0] \"Regular meditation reduces stress and anxiety.\"\n[1] \"Meditation can improve focus and attention span.\"\n\nAnswer: \"Meditation has several health benefits, including reduced stress and anxiety and improved focus. It also helps with depression and improves sleep quality.\"\n\nEvaluation:\n- \"Reduced stress and anxiety\": Faithful (from chunk 0)\n- \"Improved focus\": Faithful (from chunk 1)\n- \"Helps with depression\": Unfaithful (not mentioned in context)\n- \"Improves sleep quality\": Unfaithful (not mentioned in context)\nFaithfulness Score: 2/4 = 0.5\n</code></pre></p>"},{"location":"metrics/systematic-decomposition/#3-question-answer-answer-relevance-aq","title":"3. Question \u2192 Answer: Answer Relevance (A|Q)","text":"<p>This evaluates how well the answer addresses the question.</p> <p>Also known as: - Response Relevance - Query-Response Relevance - Answer Pertinence</p> <p>Metrics in this dimension:</p> <ul> <li>Answer Relevance: Does the answer directly address what was asked?</li> <li>Answer Completeness: Does the answer fully address all aspects of the question?</li> <li>Answer Correctness: Is the information in the answer factually accurate?</li> <li>Answer Conciseness: Is the answer appropriately detailed without unnecessary information?</li> </ul> <p>Example metric: - Semantic similarity between question and answer (Deepeval +10) - In RAG Evals: <code>AnswerRelevance</code> class in <code>metrics/relevance.py</code></p> <p>Example: <pre><code>Question: \"What are the health benefits of meditation?\"\nAnswer: \"Meditation offers numerous health benefits, including stress reduction, improved concentration, lower blood pressure, and better emotional well-being. Regular practice can help manage anxiety and depression symptoms.\"\n\nEvaluation:\n- Highly relevant (directly addresses health benefits)\n- Complete (covers multiple aspects of health benefits)\n- Concise (provides appropriate detail without meandering)\nRelevance Score: 0.95\n</code></pre></p>"},{"location":"metrics/systematic-decomposition/#4-context-chunks-answer-context-utilization","title":"4. Context Chunks \u2194 Answer: Context Utilization","text":"<p>This evaluates whether the context was effectively used in generating the answer and whether the answer reflects the context.</p> <p>Also known as: - Chunk Utility - Context Usage - Information Extraction</p> <p>Metrics in this dimension:</p> <ul> <li>Context Coverage: How much of the relevant context was utilized in the answer?</li> <li>Chunk Utility: Was each individual chunk utilized in the answer?</li> </ul> <p>Example: <pre><code>Context Chunks:\n[0] \"Regular meditation reduces stress and anxiety.\"\n[1] \"Meditation can improve focus and attention span.\"\n[2] \"The history of meditation dates back to ancient civilizations.\"\n\nAnswer: \"Meditation reduces stress and improves focus.\"\n\nEvaluation:\n- Chunk 0: Utilized\n- Chunk 1: Utilized\n- Chunk 2: Not utilized\nContext Utilization Score: 2/3 = 0.67\n</code></pre></p>"},{"location":"metrics/systematic-decomposition/#5-question-answer-question-answering-quality","title":"5. Question \u2194 Answer: Question Answering Quality","text":"<p>This evaluates the overall question-answering quality, regardless of the specific context used.</p> <p>Also known as: - Answer Quality - Response Quality - QA Performance</p> <p>Metrics in this dimension:</p> <ul> <li>Answer Accuracy: Is the answer factually correct for the question?</li> <li>Answer Helpfulness: Does the answer provide useful information to the user?</li> </ul> <p>Example: <pre><code>Question: \"What are the health benefits of meditation?\"\nAnswer: \"Meditation offers numerous health benefits, including stress reduction, improved concentration, lower blood pressure, and better emotional well-being.\"\n\nEvaluation:\n- Accurate (provides correct health benefits)\n- Helpful (gives clear, useful information)\nAccuracy Score: 0.9\n</code></pre></p>"},{"location":"metrics/systematic-decomposition/#combined-evaluation-framework","title":"Combined Evaluation Framework","text":"<p>A comprehensive RAG evaluation should assess all these relationships. This gives a complete picture of system performance:</p> <ol> <li>Retriever Performance: Question \u2192 Context evaluation (Context Relevance)</li> <li>Generator Performance: Context \u2192 Answer evaluation (Faithfulness/Groundedness)</li> <li>End-to-End Performance: Question \u2192 Answer evaluation (Answer Relevance)</li> </ol>"},{"location":"metrics/systematic-decomposition/#implementation-example","title":"Implementation Example","text":"<p>Here's how you might implement a comprehensive evaluation:</p> <pre><code># Conceptual example of a comprehensive evaluation\ndef evaluate_rag_system(question, retrieved_context, generated_answer, ground_truth=None):\n    # Retriever evaluation - Context Relevance (C|Q)\n    context_precision = evaluate_context_precision(question, retrieved_context)\n    context_recall = evaluate_context_recall(retrieved_context, ground_truth) if ground_truth else None\n\n    # Generator evaluation - Faithfulness/Groundedness (A|C)\n    faithfulness = evaluate_faithfulness(retrieved_context, generated_answer)\n\n    # End-to-end evaluation - Answer Relevance (A|Q)\n    answer_relevance = evaluate_answer_relevance(question, generated_answer)\n\n    return {\n        \"retriever_performance\": {\n            \"context_precision\": context_precision,\n            \"context_recall\": context_recall\n        },\n        \"generator_performance\": {\n            \"faithfulness\": faithfulness\n        },\n        \"end_to_end_performance\": {\n            \"answer_relevance\": answer_relevance\n        }\n    }\n</code></pre>"},{"location":"metrics/systematic-decomposition/#relationship-to-rag-evals-implementation","title":"Relationship to RAG Evals Implementation","text":"<p>The RAG Evals library implements all three key relationships:</p> <ol> <li>Context Relevance (C|Q): Implemented as <code>ChunkPrecision</code> in <code>metrics/precision.py</code></li> <li> <p>Evaluates how well each context chunk matches the question</p> </li> <li> <p>Faithfulness/Groundedness (A|C): Implemented as <code>Faithfulness</code> in <code>metrics/faithfulness.py</code></p> </li> <li> <p>Evaluates how factually consistent the answer is with the provided context</p> </li> <li> <p>Answer Relevance (A|Q): Implemented as <code>AnswerRelevance</code> in <code>metrics/relevance.py</code></p> </li> <li>Evaluates how well the answer addresses the original question</li> </ol>"},{"location":"metrics/systematic-decomposition/#advantages-of-systematic-decomposition","title":"Advantages of Systematic Decomposition","text":"<p>Breaking down RAG evaluation into these component relationships offers several advantages:</p> <ol> <li>Targeted Improvements: Identify which specific component needs improvement</li> <li>Comprehensive Assessment: Evaluate all aspects of the RAG pipeline</li> <li>Diagnostic Power: Pinpoint exact causes of system failures</li> <li>Component-wise Optimization: Optimize retriever and generator independently</li> </ol> <p>By systematically evaluating each relationship between question, context, and answer, we can build more effective and reliable RAG systems.</p>"},{"location":"usage/","title":"RAG Evals Usage Guide","text":"<p>This guide covers how to use RAG Evals to evaluate your RAG (Retrieval Augmented Generation) systems.</p>"},{"location":"usage/#installation","title":"Installation","text":"<pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"usage/#basic-usage","title":"Basic Usage","text":"<p>Here's how to perform a basic evaluation using RAG Evals:</p> <pre><code>import instructor\nfrom rag_evals.score_faithfulness import Faithfulness\nfrom rag_evals.score_precision import ChunkPrecision\n\n# Initialize with your preferred LLM\nclient = instructor.from_provider(\"openai/gpt-4o-mini\")\n\n# Define your evaluation inputs\nquestion = \"What are the benefits of exercise?\"\nanswer = \"Regular exercise improves cardiovascular health and increases strength.\"\ncontext = [\n    \"Regular physical activity improves heart health and circulation.\",\n    \"Weight training builds muscle strength and increases bone density.\",\n    \"The earliest Olympic games were held in Ancient Greece.\"\n]\n\n# Run evaluations\nfaithfulness_result = Faithfulness.grade(\n    question=question,\n    answer=answer,\n    context=context,\n    client=client\n)\n\nprecision_result = ChunkPrecision.grade(\n    question=question,\n    answer=answer,\n    context=context,\n    client=client\n)\n</code></pre>"},{"location":"usage/#parallel-execution","title":"Parallel Execution","text":"<p>For better performance, you can run evaluations in parallel:</p> <pre><code>import asyncio\nfrom instructor import AsyncInstructor\n\nasync_client = AsyncInstructor(...)\n\nasync def run_evals():\n    # Run multiple evaluations in parallel\n    faithfulness_task = Faithfulness.agrade(\n        question=question,\n        answer=answer,\n        context=context,\n        client=async_client\n    )\n\n    precision_task = ChunkPrecision.agrade(\n        question=question,\n        answer=answer,\n        context=context,\n        client=async_client\n    )\n\n    # Await results\n    faithfulness_result, precision_result = await asyncio.gather(\n        faithfulness_task, \n        precision_task\n    )\n\n    return faithfulness_result, precision_result\n\n# Run the async function\nfaithfulness_result, precision_result = asyncio.run(run_evals())\n</code></pre>"},{"location":"usage/#customizing-prompts","title":"Customizing Prompts","text":"<p>All evaluation logic is defined in prompts that you can easily modify:</p> <pre><code># Modify the faithfulness prompt\nfrom rag_evals.score_faithfulness import Faithfulness, FaithfulnessResult\nfrom rag_evals import base\n\n# Access the original prompt\noriginal_prompt = Faithfulness.prompt\n\n# Create a customized evaluator with your own prompt\nCustomFaithfulness = base.ContextEvaluation(\n    prompt=\"Your custom prompt here...\",\n    response_model=FaithfulnessResult\n)\n</code></pre>"},{"location":"usage/#working-with-different-llms","title":"Working with Different LLMs","text":"<p>RAG Evals is designed to work with any LLM that's compatible with Instructor:</p> <pre><code># Using OpenAI\nopenai_client = instructor.from_provider(\"openai/gpt-4-turbo\")\n\n# Using Anthropic\nanthropic_client = instructor.from_provider(\"anthropic/claude-3-opus\")\n\n# Using a local model\nlocal_client = instructor.from_provider(\"local/mistral-large\")\n</code></pre>"},{"location":"usage/#advanced-usage","title":"Advanced Usage","text":"<p>For more advanced usage, check out:</p> <ul> <li>Customization Guide - Learn how to customize the evaluation logic</li> <li>Best Practices - Tips for effective RAG evaluation</li> <li>Examples - Real-world examples of RAG Evals in action</li> <li>API Reference - Detailed API documentation</li> </ul>"},{"location":"usage/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter issues:</p> <ol> <li>Ensure your LLM has sufficient context window for the evaluation</li> <li>Check that your prompts are clear and provide sufficient guidance</li> <li>Verify that your context chunks are properly formatted</li> <li>Ensure you're using a compatible version of Instructor</li> </ol> <p>For more help, see the troubleshooting guide or open an issue on GitHub.</p>"},{"location":"usage/best_practices/","title":"RAG Evaluation Best Practices","text":"<p>This document provides guidance on effectively using the RAG Evals framework for Retrieval Augmented Generation (RAG) systems.</p>"},{"location":"usage/best_practices/#core-evaluation-approach","title":"Core Evaluation Approach","text":"<p>When evaluating RAG systems, we recommend these fundamental principles:</p> <ol> <li>Evaluate Multiple Dimensions: Assess both the retrieval and generation components</li> <li>Use Independent Metrics: Apply separate metrics for each aspect of performance</li> <li>Contextual Assessment: Evaluate answers within the context of the specific question and retrieved information</li> <li>Structured Evaluation: Break down complex assessments into atomic verification tasks</li> </ol>"},{"location":"usage/best_practices/#metric-specific-best-practices","title":"Metric-Specific Best Practices","text":""},{"location":"usage/best_practices/#faithfulness-evaluation","title":"Faithfulness Evaluation","text":"<ul> <li>Atomic Claims: Break answers into individual, verifiable statements</li> <li>Evidence Citation: Require specific attribution to context chunks for each claim</li> <li>Strict Criteria: All aspects of a claim must be verifiable in the context</li> <li>Tracking Support: Keep detailed records of which context chunks support which statements</li> </ul> <pre><code># Example: Detailed statement analysis\nfor statement in faithfulness_result.statements:\n    print(f\"- {statement.statement}\")\n    print(f\"  Supported: {statement.is_supported}\")\n    if statement.is_supported:\n        print(f\"  Supporting chunks: {statement.supporting_chunk_ids}\")\n        # Examine these chunks to understand the evidence base\n</code></pre>"},{"location":"usage/best_practices/#context-precision-evaluation","title":"Context Precision Evaluation","text":"<ul> <li>Independent Chunk Assessment: Evaluate each context chunk individually</li> <li>Focus on Relevance: Determine if the chunk's information is relevant to the original question</li> <li>Binary Judgment: Make a clear yes/no decision about chunk relevance</li> <li>Analysis of Irrelevant Chunks: Examine chunks marked as irrelevant to improve retrieval</li> </ul> <pre><code># Example: Analyzing irrelevant chunks to improve retrieval\nirrelevant_chunks = [chunk for chunk in precision_result.graded_chunks if not chunk.score]\nprint(f\"Number of irrelevant chunks: {len(irrelevant_chunks)}\")\nprint(f\"Proportion of relevant chunks: {precision_result.avg_score:.2f}\")\n\n# Examine why these chunks weren't relevant\nfor chunk_score in irrelevant_chunks:\n    chunk_id = chunk_score.id_chunk\n    chunk_content = context[chunk_id]\n    print(f\"Irrelevant chunk {chunk_id}: {chunk_content[:100]}...\")\n</code></pre>"},{"location":"usage/best_practices/#implementation-recommendations","title":"Implementation Recommendations","text":""},{"location":"usage/best_practices/#model-selection","title":"Model Selection","text":"<ul> <li>Evaluation Model Quality: Use the most capable model available for evaluation</li> <li>Model Separation: Consider using different models for generation vs. evaluation</li> <li>Capability Match: Ensure the evaluation model can handle the complexity of your evaluation tasks</li> </ul>"},{"location":"usage/best_practices/#context-handling","title":"Context Handling","text":"<ul> <li>Chunk Boundaries: Pay attention to how information is split across chunks</li> <li>Include IDs: Always track chunk IDs to maintain traceability</li> <li>Consistent Chunking: Use a consistent chunking strategy across your evaluation dataset</li> </ul>"},{"location":"usage/best_practices/#scoring-methodology","title":"Scoring Methodology","text":"<ul> <li>Binary + Nuance: Combine binary decisions with nuanced assessments</li> <li>Aggregation: Consider both per-example and dataset-level metrics</li> <li>Multiple Runs: For non-deterministic evaluations, consider averaging across multiple runs</li> </ul>"},{"location":"usage/best_practices/#evaluation-workflow","title":"Evaluation Workflow","text":"<ol> <li>Prepare Evaluation Data: Create a diverse set of test questions</li> <li>Run Baseline Evaluations: Establish performance benchmarks</li> <li>Analyze Failures: Identify patterns in low-scoring examples</li> <li>Implement Improvements: Based on failure analysis</li> <li>Re-evaluate: Measure improvement against the baseline</li> </ol>"},{"location":"usage/best_practices/#common-pitfalls-to-avoid","title":"Common Pitfalls to Avoid","text":"<ul> <li>Overly Generous Evaluation: Maintain strict standards for what counts as \"supported\" or \"relevant\"</li> <li>Context Leakage: Ensure evaluation doesn't use information outside the provided context</li> <li>Cherry-picking Examples: Use a representative set of test cases</li> <li>Misleading Aggregation: Report both average scores and score distributions</li> </ul>"},{"location":"usage/best_practices/#advanced-evaluation-techniques","title":"Advanced Evaluation Techniques","text":"<ul> <li>Ablation Studies: Remove components to measure their impact</li> <li>Comparative Evaluation: Test multiple RAG configurations side-by-side</li> <li>Human-in-the-loop Calibration: Periodically compare LLM evaluations with human judgments</li> <li>Multi-metric Scoring: Develop composite scores that combine multiple metrics</li> </ul>"},{"location":"usage/best_practices/#distinguishing-related-metrics","title":"Distinguishing Related Metrics","text":"<p>Understanding the relationships between different metrics is crucial:</p> <ol> <li>Context Precision vs. Chunk Utility:</li> <li>Context Precision: Measures if a chunk is relevant to the question</li> <li> <p>Chunk Utility: Measures if a chunk was actually used in the answer</p> </li> <li> <p>Faithfulness vs. Answer Relevancy:</p> </li> <li>Faithfulness: Measures if the answer is supported by the context</li> <li>Answer Relevancy: Measures if the answer addresses the question</li> </ol> <p>By following these best practices, you can develop a robust evaluation framework that helps identify and address issues in your RAG system, leading to consistent improvements in performance.</p>"},{"location":"usage/customization/","title":"Customizing RAG Evals","text":"<p>RAG Evals is designed to be flexible and customizable. This guide explains how to tailor the evaluation process to your specific needs.</p>"},{"location":"usage/customization/#customizing-prompts","title":"Customizing Prompts","text":"<p>The most straightforward way to customize RAG Evals is by modifying the evaluation prompts:</p> <pre><code>from rag_evals import base\nfrom rag_evals.score_faithfulness import FaithfulnessResult\n\n# Access the original prompt\nfrom rag_evals.score_faithfulness import Faithfulness\noriginal_prompt = Faithfulness.prompt\n\n# Create a customized evaluator with your own prompt\nCustomFaithfulness = base.ContextEvaluation(\n    prompt=\"\"\"\n    You are an expert evaluator assessing the factual accuracy of answers.\n\n    For each statement in the answer:\n    1. Determine if it is fully supported by the context\n    2. Be extremely strict - only mark as supported if there is explicit evidence\n    3. Identify which context chunks provide the supporting evidence\n\n    Your evaluation should be thorough and rigorous.\n    \"\"\",\n    response_model=FaithfulnessResult\n)\n</code></pre>"},{"location":"usage/customization/#prompt-design-tips","title":"Prompt Design Tips","text":"<p>When customizing prompts, consider these best practices:</p> <ol> <li>Be Explicit: Clearly define what constitutes a supported vs. unsupported statement</li> <li>Set the Evaluation Tone: Indicate how strict the evaluation should be</li> <li>Specify Output Format: Ensure the prompt produces output compatible with the response model</li> <li>Provide Examples: For complex evaluations, include examples in the prompt</li> </ol>"},{"location":"usage/customization/#customizing-response-models","title":"Customizing Response Models","text":"<p>You can also create custom response models for more specialized evaluations:</p> <pre><code>from pydantic import BaseModel, Field\nfrom rag_evals import base\n\n# Define a custom response model\nclass EnhancedStatementEvaluation(BaseModel):\n    statement: str = Field(description=\"An individual claim from the answer\")\n    is_supported: bool = Field(description=\"Whether the statement is supported\")\n    supporting_chunk_ids: list[int] = Field(default_factory=list)\n    confidence_level: str = Field(\n        description=\"Confidence level of the support assessment\",\n        default=\"high\"\n    )\n\nclass EnhancedFaithfulnessResult(BaseModel):\n    statements: list[EnhancedStatementEvaluation]\n\n    @property\n    def overall_faithfulness_score(self) -&gt; float:\n        if not self.statements:\n            return 0.0\n        supported_statements = sum(s.is_supported for s in self.statements)\n        return supported_statements / len(self.statements)\n\n    @property\n    def high_confidence_score(self) -&gt; float:\n        \"\"\"Score based only on high-confidence assessments\"\"\"\n        high_confidence = [s for s in self.statements if s.confidence_level == \"high\"]\n        if not high_confidence:\n            return 0.0\n        supported = sum(s.is_supported for s in high_confidence)\n        return supported / len(high_confidence)\n\n# Create an evaluator that uses this model\nEnhancedFaithfulness = base.ContextEvaluation(\n    prompt=\"\"\"\n    You are an expert evaluator assessing the factual accuracy of answers.\n\n    For each statement in the answer:\n    1. Determine if it is supported by the context\n    2. Assign a confidence level (high, medium, low) to your assessment\n    3. Identify which context chunks provide the supporting evidence\n\n    Output a structured evaluation including all these elements.\n    \"\"\",\n    response_model=EnhancedFaithfulnessResult\n)\n</code></pre>"},{"location":"usage/customization/#creating-new-metrics","title":"Creating New Metrics","text":"<p>You can create entirely new evaluation metrics by defining new evaluation classes:</p> <pre><code>from pydantic import BaseModel, Field\nfrom rag_evals import base\n\n# Define a response model for answer relevancy\nclass RelevancyResult(BaseModel):\n    topical_match: float = Field(ge=0.0, le=1.0, description=\"Score for how well the answer topic matches the question\")\n    completeness: float = Field(ge=0.0, le=1.0, description=\"Score for how completely the answer addresses all aspects of the question\")\n    specificity: float = Field(ge=0.0, le=1.0, description=\"Score for how specific and detailed the answer is\")\n\n    @property\n    def overall_relevancy_score(self) -&gt; float:\n        # Weighted average of the component scores\n        return (0.4 * self.topical_match + \n                0.4 * self.completeness + \n                0.2 * self.specificity)\n\n# Create the relevancy evaluator\nAnswerRelevancy = base.ContextEvaluation(\n    prompt=\"\"\"\n    You are an expert evaluator assessing how well an answer addresses the original question.\n\n    Evaluate the answer along three dimensions:\n\n    1. Topical Match (0.0-1.0): How well does the answer's subject matter align with what was asked?\n    2. Completeness (0.0-1.0): How thoroughly does the answer address all aspects of the question?\n    3. Specificity (0.0-1.0): How specific and detailed is the answer relative to what was asked?\n\n    Provide scores for each dimension and explain your reasoning.\n    \"\"\",\n    response_model=RelevancyResult\n)\n</code></pre>"},{"location":"usage/customization/#customizing-the-evaluation-process","title":"Customizing the Evaluation Process","text":"<p>The <code>ContextEvaluation</code> class provides a flexible foundation for custom evaluation processes:</p>"},{"location":"usage/customization/#custom-templates","title":"Custom Templates","text":"<p>You can customize the chunk template used for evaluation:</p> <pre><code>from rag_evals import base\nfrom textwrap import dedent\n\n# Create an evaluator with a custom template\nCustomEvaluator = base.ContextEvaluation(\n    prompt=\"Your evaluation prompt here...\",\n    response_model=YourResponseModel,\n    chunk_template=dedent(\"\"\"\n        &lt;custom_evaluation&gt;\n            &lt;query&gt;{{ question }}&lt;/query&gt;\n            &lt;response&gt;{{ answer }}&lt;/response&gt;\n            &lt;retrieved_documents&gt;\n                {% for chunk in context %}\n                &lt;document id=\"{{ chunk.id }}\"&gt;\n                    {{ chunk.chunk }}\n                &lt;/document&gt;\n                {% endfor %}\n            &lt;/retrieved_documents&gt;\n        &lt;/custom_evaluation&gt;\n    \"\"\")\n)\n</code></pre>"},{"location":"usage/customization/#adding-examples","title":"Adding Examples","text":"<p>You can provide examples to guide the evaluation:</p> <pre><code># Define examples\nexamples = [\n    {\n        \"question\": \"What is photosynthesis?\",\n        \"answer\": \"Photosynthesis is the process by which plants convert sunlight into energy.\",\n        \"context\": [\"Photosynthesis is the process by which plants use sunlight to create energy.\"],\n        \"expected_result\": {\"faithfulness_score\": 1.0}\n    },\n    {\n        \"question\": \"Who was Albert Einstein?\",\n        \"answer\": \"Einstein was a physicist who developed the theory of relativity and won a Nobel Prize.\",\n        \"context\": [\"Albert Einstein was a theoretical physicist known for developing the theory of relativity.\"],\n        \"expected_result\": {\"faithfulness_score\": 0.5}\n    }\n]\n\n# Create an evaluator with examples\nExampleGuidedEvaluator = base.ContextEvaluation(\n    prompt=\"Your evaluation prompt here...\",\n    response_model=YourResponseModel,\n    examples=examples\n)\n</code></pre>"},{"location":"usage/customization/#integration-with-custom-llms","title":"Integration with Custom LLMs","text":"<p>You can use any Instructor-compatible LLM provider for evaluations:</p> <pre><code>import instructor\nfrom rag_evals.score_faithfulness import Faithfulness\n\n# Using OpenAI\nopenai_client = instructor.from_provider(\"openai/gpt-4-turbo\")\n\n# Using Anthropic\nanthropic_client = instructor.from_provider(\"anthropic/claude-3-opus\")\n\n# Using Cohere\ncohere_client = instructor.from_provider(\"cohere/command-r\")\n\n# Using a local model\nlocal_client = instructor.from_provider(\"local/llama-3-70b\")\n\n# Run evaluation with your preferred client\nresult = Faithfulness.grade(\n    question=question,\n    answer=answer,\n    context=context,\n    client=your_client_of_choice\n)\n</code></pre> <p>By leveraging these customization options, you can adapt RAG Evals to a wide range of evaluation needs and scenarios.</p>"},{"location":"usage/examples/","title":"RAG Evals Examples","text":"<p>This page provides practical examples of using RAG Evals in different scenarios.</p>"},{"location":"usage/examples/#basic-usage-example","title":"Basic Usage Example","text":"<p>This example shows how to evaluate a simple RAG system using faithfulness, context precision, and answer relevance metrics:</p> <pre><code>import instructor\nfrom rag_evals import Faithfulness, ChunkPrecision, AnswerRelevance\n\n# Initialize with LLM\nclient = instructor.from_provider(\"openai/gpt-4o-mini\")\n\n# Sample RAG output\nquestion = \"What are the benefits of exercise?\"\nanswer = \"Regular exercise improves cardiovascular health and increases strength.\"\ncontext = [\n    \"Regular physical activity improves heart health and circulation.\",\n    \"Weight training builds muscle strength and increases bone density.\",\n    \"The earliest Olympic games were held in Ancient Greece.\"\n]\n\n# Evaluate faithfulness\nfaithfulness_result = Faithfulness.grade(\n    question=question,\n    answer=answer,\n    context=context,\n    client=client\n)\n\n# Evaluate context precision\nprecision_result = ChunkPrecision.grade(\n    question=question,\n    answer=answer,\n    context=context,\n    client=client\n)\n\n# Evaluate answer relevance\nrelevance_result = AnswerRelevance.grade(\n    question=question,\n    answer=answer,\n    context=context,  # Context isn't used in relevance evaluation\n    client=client\n)\n\n# Process and display results\nprint(f\"Overall Faithfulness Score: {faithfulness_result.overall_faithfulness_score:.2f}\")\nprint(f\"Overall Context Precision: {precision_result.avg_score:.2f}\")\nprint(f\"Overall Answer Relevance: {relevance_result.overall_score:.2f}\")\n\n# Detailed analysis\nprint(\"\\nFaithfulness Breakdown:\")\nfor statement in faithfulness_result.statements:\n    print(f\"- {statement.statement}\")\n    print(f\"  Supported: {statement.is_supported}\")\n    if statement.is_supported and statement.supporting_chunk_ids:\n        print(f\"  Supporting chunks: {statement.supporting_chunk_ids}\")\n\nprint(\"\\nContext Precision Breakdown:\")\nfor chunk in precision_result.graded_chunks:\n    print(f\"- Chunk {chunk.id_chunk}: {'Relevant' if chunk.score else 'Not Relevant'}\")\n\nprint(\"\\nAnswer Relevance Breakdown:\")\nprint(f\"- Topical Match: {relevance_result.topical_match:.2f}\")\nprint(f\"- Completeness: {relevance_result.completeness:.2f}\")\nprint(f\"- Conciseness: {relevance_result.conciseness:.2f}\")\nprint(f\"- Reasoning: {relevance_result.reasoning}\")\n</code></pre>"},{"location":"usage/examples/#parallel-evaluation-example","title":"Parallel Evaluation Example","text":"<p>This example demonstrates how to run multiple evaluations in parallel for better performance:</p> <pre><code>import asyncio\nimport instructor\nfrom rag_evals import Faithfulness, ChunkPrecision, AnswerRelevance\n\n# Initialize with async client\nasync_client = instructor.AsyncInstructor(provider=\"openai/gpt-4o-mini\")\n\n# Sample RAG outputs to evaluate\nexamples = [\n    {\n        \"question\": \"What are the benefits of exercise?\",\n        \"answer\": \"Regular exercise improves cardiovascular health and increases strength.\",\n        \"context\": [\n            \"Regular physical activity improves heart health and circulation.\",\n            \"Weight training builds muscle strength and increases bone density.\",\n            \"The earliest Olympic games were held in Ancient Greece.\"\n        ]\n    },\n    {\n        \"question\": \"How does photosynthesis work?\",\n        \"answer\": \"Photosynthesis is the process by which plants convert sunlight into energy.\",\n        \"context\": [\n            \"Photosynthesis is the process by which plants use sunlight to synthesize foods from carbon dioxide and water.\",\n            \"The process primarily happens in the plant's leaves through their chloroplasts.\",\n            \"Solar panels are designed to convert sunlight into electricity.\"\n        ]\n    }\n]\n\nasync def evaluate_example(example):\n    \"\"\"Evaluate a single example using multiple metrics in parallel\"\"\"\n    faithfulness_task = Faithfulness.agrade(\n        question=example[\"question\"],\n        answer=example[\"answer\"],\n        context=example[\"context\"],\n        client=async_client\n    )\n\n    precision_task = ChunkPrecision.agrade(\n        question=example[\"question\"],\n        answer=example[\"answer\"],\n        context=example[\"context\"],\n        client=async_client\n    )\n\n    relevance_task = AnswerRelevance.agrade(\n        question=example[\"question\"],\n        answer=example[\"answer\"],\n        context=example[\"context\"],\n        client=async_client\n    )\n\n    # Run all evaluations in parallel\n    faithfulness_result, precision_result, relevance_result = await asyncio.gather(\n        faithfulness_task, precision_task, relevance_task\n    )\n\n    return {\n        \"question\": example[\"question\"],\n        \"faithfulness_score\": faithfulness_result.overall_faithfulness_score,\n        \"precision_score\": precision_result.avg_score,\n        \"relevance_score\": relevance_result.overall_score\n    }\n\nasync def evaluate_all_examples():\n    \"\"\"Evaluate all examples in parallel\"\"\"\n    tasks = [evaluate_example(example) for example in examples]\n    results = await asyncio.gather(*tasks)\n    return results\n\n# Run the evaluations\nresults = asyncio.run(evaluate_all_examples())\n\n# Display results\nfor result in results:\n    print(f\"\\nQuestion: {result['question']}\")\n    print(f\"Faithfulness Score: {result['faithfulness_score']:.2f}\")\n    print(f\"Precision Score: {result['precision_score']:.2f}\")\n    print(f\"Relevance Score: {result['relevance_score']:.2f}\")\n</code></pre>"},{"location":"usage/examples/#custom-prompt-example","title":"Custom Prompt Example","text":"<p>This example shows how to customize the evaluation prompts:</p> <pre><code>from rag_evals import base, FaithfulnessResult, RelevanceScore, ChunkGradedBinary\n\n# Define a custom answer relevance evaluator with a modified prompt\nCustomRelevance = base.ContextEvaluation(\n    prompt=\"\"\"\n    You are evaluating how well an answer addresses the question.\n\n    Evaluate the answer along these dimensions:\n    1. Topical relevance (0-1): Does the answer address the topic of the question?\n    2. Completeness (0-1): Does the answer cover all parts of the question?\n    3. Conciseness (0-1): Is the answer appropriately detailed without unnecessary information?\n\n    Give each dimension a score and provide your reasoning.\n    \"\"\",\n    response_model=RelevanceScore\n)\n\n# Define a custom context precision evaluator\nCustomPrecision = base.ContextEvaluation(\n    prompt=\"\"\"\n    You are evaluating the relevance of retrieved context chunks to the original question.\n\n    For each retrieved chunk:\n    1. Determine if it contains information that would help answer the question\n    2. Consider both direct relevance (explicitly addresses the question) and indirect relevance (provides background or related information)\n    3. Be moderately strict - the chunk should contain actual helpful information, not just be on a vaguely related topic\n\n    Output a binary judgment (relevant/not relevant) for the chunk.\n    \"\"\",\n    response_model=base.ChunkGradedBinary\n)\n\n# Use the custom evaluators\nrelevance_result = CustomRelevance.grade(\n    question=question,\n    answer=answer,\n    context=context,\n    client=client\n)\n\nprecision_result = CustomPrecision.grade(\n    question=question,\n    answer=answer,\n    context=context,\n    client=client\n)\n</code></pre>"},{"location":"usage/examples/#comprehensive-evaluation-example","title":"Comprehensive Evaluation Example","text":"<p>This example demonstrates a complete evaluation framework that assesses all key relationships:</p> <pre><code>import pandas as pd\nimport instructor\nfrom rag_evals import Faithfulness, ChunkPrecision, AnswerRelevance\n\ndef evaluate_rag_system(question, context, answer, client):\n    \"\"\"Comprehensive RAG evaluation across all key dimensions\"\"\"\n\n    # 1. Context \u2192 Question: Context Relevance\n    precision_result = ChunkPrecision.grade(\n        question=question,\n        answer=answer,\n        context=context,\n        client=client\n    )\n\n    # 2. Context \u2192 Answer: Faithfulness\n    faithfulness_result = Faithfulness.grade(\n        question=question,\n        answer=answer,\n        context=context,\n        client=client\n    )\n\n    # 3. Question \u2192 Answer: Answer Relevance\n    relevance_result = AnswerRelevance.grade(\n        question=question,\n        answer=answer,\n        context=context,\n        client=client\n    )\n\n    # Calculate aggregate scores\n    retriever_score = precision_result.avg_score\n    generator_score = faithfulness_result.overall_faithfulness_score\n    end_to_end_score = relevance_result.overall_score\n\n    # Overall system score (simple average)\n    overall_score = (retriever_score + generator_score + end_to_end_score) / 3\n\n    return {\n        \"overall_score\": overall_score,\n        \"retriever_score\": retriever_score,\n        \"generator_score\": generator_score,\n        \"end_to_end_score\": end_to_end_score,\n        \"precision_details\": precision_result,\n        \"faithfulness_details\": faithfulness_result,\n        \"relevance_details\": relevance_result\n    }\n\n# Example usage\nclient = instructor.from_provider(\"openai/gpt-4o-mini\")\n\nquestion = \"What are the health benefits of meditation?\"\nanswer = \"Meditation reduces stress and improves mental focus. It also helps with depression.\"\ncontext = [\n    \"Regular meditation reduces stress and anxiety.\",\n    \"Meditation can improve focus and attention span.\",\n    \"The history of meditation dates back to ancient civilizations.\"\n]\n\nresults = evaluate_rag_system(question, context, answer, client)\n\n# Display results\nprint(f\"Overall System Score: {results['overall_score']:.2f}\")\nprint(f\"Retriever Score (Context Relevance): {results['retriever_score']:.2f}\")\nprint(f\"Generator Score (Faithfulness): {results['generator_score']:.2f}\")\nprint(f\"End-to-End Score (Answer Relevance): {results['end_to_end_score']:.2f}\")\n\n# Detailed analysis (example)\nprint(\"\\nUnfaithful statements:\")\nfor stmt in results['faithfulness_details'].statements:\n    if not stmt.is_supported:\n        print(f\"- {stmt.statement}\")\n\nprint(\"\\nIrrelevant context chunks:\")\nfor chunk in results['precision_details'].graded_chunks:\n    if not chunk.score:\n        print(f\"- Chunk {chunk.id_chunk}: {context[chunk.id_chunk][:50]}...\")\n</code></pre> <p>These examples demonstrate the flexibility and power of RAG Evals for different evaluation scenarios. You can adapt these patterns to fit your specific evaluation needs.</p>"},{"location":"usage/troubleshooting/","title":"Troubleshooting RAG Evals","text":"<p>This guide helps you troubleshoot common issues when using RAG Evals.</p>"},{"location":"usage/troubleshooting/#common-issues","title":"Common Issues","text":""},{"location":"usage/troubleshooting/#llm-api-errors","title":"LLM API Errors","text":"<p>Problem: Errors when connecting to the LLM provider.</p> <p>Possible Solutions: - Verify API key is valid and correctly set in your environment - Check API rate limits and quotas - Ensure you have the correct Instructor version for your provider - Verify network connectivity to the LLM provider</p> <pre><code># Example of proper client initialization\nimport instructor\nimport os\n\n# Ensure API key is set\nassert os.environ.get(\"OPENAI_API_KEY\"), \"API key not found in environment\"\n\n# Initialize client with error handling\ntry:\n    client = instructor.from_provider(\"openai/gpt-4o-mini\")\nexcept Exception as e:\n    print(f\"Failed to initialize client: {e}\")\n    # Handle the error appropriately\n</code></pre>"},{"location":"usage/troubleshooting/#context-window-limitations","title":"Context Window Limitations","text":"<p>Problem: Evaluation fails due to exceeding the model's context window.</p> <p>Possible Solutions: - Reduce the size of context chunks - Use a model with a larger context window - Limit the amount of context provided per evaluation - Break large evaluations into smaller batches</p> <pre><code># Example: Limiting context size\ndef limit_context_size(context, max_chunks=10, max_chunk_size=500):\n    \"\"\"Limit context to prevent exceeding context window limits\"\"\"\n    # Limit number of chunks\n    limited_context = context[:max_chunks]\n\n    # Limit size of each chunk\n    limited_context = [chunk[:max_chunk_size] for chunk in limited_context]\n\n    return limited_context\n\n# Use limited context in evaluation\nfaithfulness_result = Faithfulness.grade(\n    question=question,\n    answer=answer,\n    context=limit_context_size(context),\n    client=client\n)\n</code></pre>"},{"location":"usage/troubleshooting/#validation-errors","title":"Validation Errors","text":"<p>Problem: Errors related to the validation of response models.</p> <p>Possible Solutions: - Check if your prompt aligns with the expected response model - Verify that chunk IDs are correctly referenced - Ensure the LLM is producing output that matches the response model schema - Add more explicit instructions in your prompt about the required output format</p> <pre><code># Example: Adding explicit format guidance to prompt\nfrom rag_evals import base\nfrom rag_evals.score_faithfulness import FaithfulnessResult\n\n# Create custom evaluator with explicit format instructions\nExplicitFormatFaithfulness = base.ContextEvaluation(\n    prompt=\"\"\"\n    You are an expert evaluator assessing faithfulness.\n\n    IMPORTANT: Your output MUST follow this exact JSON structure:\n    {\n      \"statements\": [\n        {\n          \"statement\": \"The exact claim from the answer\",\n          \"is_supported\": true or false,\n          \"supporting_chunk_ids\": [list of integer IDs or null]\n        },\n        ...\n      ]\n    }\n\n    [Rest of prompt instructions...]\n    \"\"\",\n    response_model=FaithfulnessResult\n)\n</code></pre>"},{"location":"usage/troubleshooting/#incorrect-chunk-ids","title":"Incorrect Chunk IDs","text":"<p>Problem: The evaluation references chunk IDs that don't exist in the context.</p> <p>Possible Solutions: - Ensure chunk IDs start from 0 and are sequential - Verify that the LLM understands the chunk ID format - Check for chunk ID consistency in your prompts and examples - Make the consequences of invalid chunk IDs explicit in your prompt</p> <pre><code># Example: Validating context chunks before evaluation\ndef validate_context(context):\n    \"\"\"Ensure context is properly formatted with sequential IDs\"\"\"\n    if not context:\n        raise ValueError(\"Context cannot be empty\")\n\n    # Ensure context is a list\n    if not isinstance(context, list):\n        raise TypeError(\"Context must be a list of strings\")\n\n    # Check that all context items are strings\n    for i, chunk in enumerate(context):\n        if not isinstance(chunk, str):\n            raise TypeError(f\"Context chunk {i} is not a string\")\n\n    return context\n\n# Use validated context\nfaithfulness_result = Faithfulness.grade(\n    question=question,\n    answer=answer,\n    context=validate_context(context),\n    client=client\n)\n</code></pre>"},{"location":"usage/troubleshooting/#inconsistent-evaluation-results","title":"Inconsistent Evaluation Results","text":"<p>Problem: Evaluations produce inconsistent or unexpected results.</p> <p>Possible Solutions: - Use a more capable LLM for evaluation - Provide explicit scoring criteria in the prompt - Add few-shot examples to guide the evaluation - Run multiple evaluations and average the results - Review your prompt for clarity and potential ambiguities</p> <pre><code># Example: Running multiple evaluations for consistency\ndef evaluate_with_redundancy(question, answer, context, client, evaluator, n=3):\n    \"\"\"Run multiple evaluations and aggregate results for more consistency\"\"\"\n    results = []\n\n    for _ in range(n):\n        result = evaluator.grade(\n            question=question,\n            answer=answer,\n            context=context,\n            client=client\n        )\n        results.append(result)\n\n    # For faithfulness evaluations, average overall scores\n    if hasattr(results[0], 'overall_faithfulness_score'):\n        avg_score = sum(r.overall_faithfulness_score for r in results) / len(results)\n        print(f\"Average Faithfulness Score: {avg_score:.2f}\")\n\n    # For precision evaluations, average across chunk scores\n    elif hasattr(results[0], 'avg_score'):\n        avg_score = sum(r.avg_score for r in results) / len(results)\n        print(f\"Average Precision Score: {avg_score:.2f}\")\n\n    return results\n</code></pre>"},{"location":"usage/troubleshooting/#performance-issues","title":"Performance Issues","text":"<p>Problem: Evaluations are too slow.</p> <p>Possible Solutions: - Use parallel processing with <code>agrade</code> and <code>asyncio</code> - Batch evaluations when possible - Use a faster LLM for evaluations that don't require high capability - Optimize context size to reduce token count - Consider using client-side caching for repeated evaluations</p> <pre><code># Example: Parallel evaluation of multiple metrics\nimport asyncio\nfrom instructor import AsyncInstructor\nfrom rag_evals.score_faithfulness import Faithfulness\nfrom rag_evals.score_precision import ChunkPrecision\n\nasync_client = AsyncInstructor(provider=\"openai/gpt-4o-mini\")\n\nasync def evaluate_example(question, answer, context):\n    \"\"\"Run all evaluations for a single example in parallel\"\"\"\n    faithfulness_task = Faithfulness.agrade(\n        question=question,\n        answer=answer,\n        context=context,\n        client=async_client\n    )\n\n    precision_task = ChunkPrecision.agrade(\n        question=question,\n        answer=answer,\n        context=context,\n        client=async_client\n    )\n\n    # Run both evaluations in parallel\n    return await asyncio.gather(faithfulness_task, precision_task)\n\n# Use with asyncio.run(evaluate_example(...))\n</code></pre>"},{"location":"usage/troubleshooting/#model-specific-issues","title":"Model-Specific Issues","text":""},{"location":"usage/troubleshooting/#gpt-models","title":"GPT Models","text":"<ul> <li>Ensure you're using the correct model name format (e.g., \"openai/gpt-4o-mini\")</li> <li>Monitor token usage to avoid unexpected costs</li> <li>Be aware of rate limits, especially with parallel evaluations</li> </ul>"},{"location":"usage/troubleshooting/#claude-models","title":"Claude Models","text":"<ul> <li>Properly format the system and user messages for Claude</li> <li>Be aware of Claude's handling of structured output</li> <li>Adjust prompts to accommodate Claude's reasoning style</li> </ul>"},{"location":"usage/troubleshooting/#local-models","title":"Local Models","text":"<ul> <li>Ensure the model supports structured JSON output</li> <li>Be prepared for less consistent results with smaller models</li> <li>Consider using more explicit prompts for local models</li> </ul>"},{"location":"usage/troubleshooting/#debugging-tips","title":"Debugging Tips","text":"<ol> <li>Inspect Raw API Responses: Look at the raw API responses to understand what the LLM is returning</li> <li>Log Intermediate Steps: Add logging to track the evaluation process</li> <li>Test With Simple Examples: Verify functionality with simple, known examples first</li> <li>Compare With Manual Evaluation: Periodically validate results against human judgments</li> <li>Check Context Processing: Verify that context is being correctly processed and enumerated</li> </ol> <p>If you continue to experience issues, please check the project repository for known issues or submit a new issue with details about your problem.</p>"}]}